# -*- coding: utf-8 -*-
"""Inverse StyleGANXL - Training

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Jk2mKTSBYE6L7JiO2v6VarVS-oxWlex
"""

import torch
print(torch.__version__)

# !git clone https://github.com/autonomousvision/stylegan_xl.git
# !git clone https://github.com/openai/CLIP
# !pip install -e ./CLIP
# !pip install einops ninja
# !pip install timm

import sys
sys.path.append('./CLIP')
sys.path.append('./stylegan_xl')

import numpy as np
import torch
import torch.nn as nn
import tensorflow as tf
import pickle
import os

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import PIL
from PIL import Image
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
import torch.optim as optim

import dnnlib
import legacy
from torch_utils import gen_utils

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print('Using device:', device, file=sys.stderr)

# Load in the trained StyleganXL model
network_pkl = "https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet1024.pkl"
with dnnlib.util.open_url(network_pkl) as f:
        G = legacy.load_network_pkl(f)['G_ema']
        G = G.eval().requires_grad_(False).to(device)
c = None

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

def get_latents(amount, seed, shape = 512):
    latents = torch.from_numpy(np.random.RandomState(seed).randn(amount, shape)).cuda()
    w = []
    for i in range(latents.shape[0]):
        w = G.mapping(latents[i][None], c)
        if i<=0:
            w_new = w
        else:
            w_new = torch.cat((w, w_new), 0)
    return latents, w_new

def img_from_latent(net, latents, img_size, show_img = False, outdir = None):
    outputs = torch.empty((len(latents), 3, img_size, img_size))
    transform=transforms.Compose([transforms.Resize(img_size),transforms.CenterCrop(img_size),transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])
    for i, latent in enumerate(latents):
        img = gen_utils.w_to_img(net, latent, to_np=True, noise_mode='none')
        img = Image.fromarray(img[0], 'RGB')
        img = img.resize((img_size,img_size), resample = PIL.Image.LANCZOS)
        if show_img:
            plt.axis('off')
            plt.title("Image "+str(i))
            plt.imshow(img)
            plt.show()
        if not outdir is None:
          img.save(outdir + "Image %d.png" % i)
        img = transform(img)
        outputs[i] = img
    return outputs

def train(netD, netG, num_iters, batch_size, img_size, seed, criterion, optimizer):
    losses = []
    for i in range(num_iters):
        w = gen_utils.get_w_from_seed(G, batch_size, device, seed = np.random.seed(seed))
        w = w.to(device) # naar float32?
        x = img_from_latent(netG, w, img_size).to(device)

        netD.zero_grad()

        output = netD(x).squeeze()

        errD = criterion(output, w[:,0])
        errD.backward()
        optimizer.step()
        
        if i % 5 == 0:
          print('[%d/%d]\tLoss_D: %.4f' % (i+1, num_iters, errD.item()))

        losses.append(errD.item())

    return losses

class Discriminator(nn.Module):
    def __init__(self, channels = 3, feature_maps = 64):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # input is (nc) x 64 x 64
            nn.Conv2d(channels, feature_maps, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf) x 32 x 32
            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*2) x 16 x 16
            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*4) x 8 x 8
            nn.Conv2d(feature_maps * 4, feature_maps * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 4 x 4
            nn.Conv2d(feature_maps * 8, 512, 4, 1, 0, bias=False),
        )

    def forward(self, input):
        return self.main(input)

# Set number of gpu's and device
img_size = 64

netD_Adam_W = Discriminator().to(device)
# netD_Adam_W.apply(weights_init)
optimizerD_Adam_W = optim.Adam(netD_Adam_W.parameters(), lr=0.0002)
criterion = nn.MSELoss()

path = 'intermediate_results/netD_imagenet1024_w_30000.pt'
checkpoint = torch.load(path)
netD_Adam_W.load_state_dict(checkpoint['model_state_dict'])
optimizerD_Adam_W.load_state_dict(checkpoint['optimizer_state_dict'])
iter = checkpoint['iter']
loss = checkpoint['loss']

print(netD_Adam_W)
# print(iter)

# Number of training epochs
num_iters = 10000

# Set batch size, for number of generated samples per epoch
batch_size = 32

# Set random seed
seed = 42

losses_adam_w = train(netD_Adam_W, G, num_iters, batch_size, img_size, seed, criterion, optimizerD_Adam_W)

# plt.figure(figsize=(10,5))
# plt.title("Discriminator Loss During Training")
# plt.plot(losses_adam_w,label="Loss")
# plt.xlabel("Number of epochs")
# plt.ylabel("Loss")
# plt.legend()
# plt.show()

iter = 40000
path = 'intermediate_results/netD_imagenet1024_w_40000.pt'
loss = losses_adam_w[-1]

torch.save({
    'iter': iter,
    'model_state_dict': netD_Adam_W.state_dict(),
    'optimizer_state_dict': optimizerD_Adam_W.state_dict(),
    'loss': loss,
}, path)
