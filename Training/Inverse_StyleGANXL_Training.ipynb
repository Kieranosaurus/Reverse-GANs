{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyrfPJncFhBj",
        "outputId": "e449b16e-e5d7-4795-c45e-dccab4842ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/autonomousvision/stylegan_xl.git\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "ZSQT6roHzYfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e331a3cd-b350-4698-8b82-14770f9d36be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan_xl'...\n",
            "remote: Enumerating objects: 298, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 298 (delta 75), reused 80 (delta 53), pack-reused 192\u001b[K\n",
            "Receiving objects: 100% (298/298), 13.89 MiB | 26.23 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Total 236 (delta 0), reused 0 (delta 0), pack-reused 236\u001b[K\n",
            "Receiving objects: 100% (236/236), 8.92 MiB | 20.29 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/CLIP\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.0+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 370 kB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja, einops\n",
            "Successfully installed einops-0.6.0 ninja-1.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[K     |████████████████████████████████| 549 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm) (4.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2.10)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.11.1 timm-0.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYEq2XszkPjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77fb8fe-15e1-4362-eeae-72546e9d0dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan_xl')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch.optim as optim\n",
        "\n",
        "import dnnlib\n",
        "import legacy\n",
        "from torch_utils import gen_utils\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the trained StyleganXL model\n",
        "network_pkl = \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet1024.pkl\"\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "        G = legacy.load_network_pkl(f)['G_ema']\n",
        "        G = G.eval().requires_grad_(False).to(device)\n",
        "c = None"
      ],
      "metadata": {
        "id": "rDNsT9BwdqO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "def get_latents(amount, seed, shape = 512):\n",
        "    latents = torch.from_numpy(np.random.RandomState(seed).randn(amount, shape)).cuda()\n",
        "    w = torch.empty(amount, 16, shape)\n",
        "    for i in range(latents.shape[0]):\n",
        "        w[i] = G.mapping(latents[i][None], c)\n",
        "    return latents, w\n",
        "\n",
        "def img_from_latent(net, latents, img_size, show_img = False, outdir = None):\n",
        "    outputs = torch.empty((len(latents), 3, img_size, img_size))\n",
        "    transform=transforms.Compose([transforms.Resize(img_size),transforms.CenterCrop(img_size),transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
        "    for i, latent in enumerate(latents):\n",
        "        img = gen_utils.w_to_img(net, latent, to_np=True, noise_mode='none')\n",
        "        img = Image.fromarray(img[0], 'RGB')\n",
        "        img = img.resize((img_size,img_size), resample = PIL.Image.LANCZOS)\n",
        "        if show_img:\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Image \"+str(i))\n",
        "            plt.imshow(img)\n",
        "            plt.show()\n",
        "        if not outdir is None:\n",
        "          img.save(outdir + \"Image %d.png\" % i)\n",
        "        img = transform(img)\n",
        "        outputs[i] = img\n",
        "    return outputs\n",
        "\n",
        "def train(netD, netG, num_iters, batch_size, img_size, seed, criterion, optimizer):\n",
        "    losses = []\n",
        "    for i in range(num_iters):\n",
        "        w = gen_utils.get_w_from_seed(G, batch_size, device, seed = np.random.seed(seed))\n",
        "        w = w.to(device) # naar float32?\n",
        "        x = img_from_latent(netG, w, img_size).to(device)\n",
        "\n",
        "        netD.zero_grad()\n",
        "\n",
        "        output = netD(x).squeeze()\n",
        "\n",
        "        errD = criterion(output, w[:,0])\n",
        "        errD.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i % 5 == 0:\n",
        "          print('[%d/%d]\\tLoss_D: %.4f' % (i+1, num_iters, errD.item()))\n",
        "\n",
        "        losses.append(errD.item())\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "dX1PpzKI6uVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels = 3, feature_maps = 64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(channels, feature_maps, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(feature_maps * 4, feature_maps * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_maps * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(feature_maps * 8, 512, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "nCR2zBjW1V6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of gpu's and device\n",
        "img_size = 64"
      ],
      "metadata": {
        "id": "ZpoSa96c6ykc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "outdir = '/output/'\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "batch_sz = 32\n",
        "\n",
        "w = gen_utils.get_w_from_seed(G, batch_sz, device, seed=np.random.seed(seed))\n",
        "w = w.to(device)\n",
        "print(w.shape)\n",
        "\n",
        "outputs = img_from_latent(G, w, 1024, outdir=outdir)"
      ],
      "metadata": {
        "id": "4ow_vgt3xnpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netD_Adam_W = Discriminator().to(device)\n",
        "# netD_Adam_W.apply(weights_init)\n",
        "optimizerD_Adam_W = optim.Adam(netD_Adam_W.parameters(), lr=0.0002)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "path = '/content/drive/MyDrive/Internship/training/netD_imagenet1024_w_1500.pt'\n",
        "checkpoint = torch.load(path)\n",
        "netD_Adam_W.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizerD_Adam_W.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "iter = checkpoint['iter']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "print(netD_Adam_W)\n",
        "print(iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG2Iipi9wyYB",
        "outputId": "4c2f4efd-439a-4b77-f015-730c8575bbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training epochs\n",
        "num_iters = 500\n",
        "\n",
        "# Set batch size, for number of generated samples per epoch\n",
        "batch_size = 32\n",
        "\n",
        "# Set random seed\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "HN09lrVcPfHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_adam_w = train(netD_Adam_W, G, num_iters, batch_size, img_size, seed, criterion, optimizerD_Adam_W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxZLalmCWDdc",
        "outputId": "e6e85989-f43a-48a7-9b6f-6e2afd93b05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/500]\tLoss_D: 0.5566\n",
            "[6/500]\tLoss_D: 0.5636\n",
            "[11/500]\tLoss_D: 0.5556\n",
            "[16/500]\tLoss_D: 0.5481\n",
            "[21/500]\tLoss_D: 0.5791\n",
            "[26/500]\tLoss_D: 0.6156\n",
            "[31/500]\tLoss_D: 0.5576\n",
            "[36/500]\tLoss_D: 0.5675\n",
            "[41/500]\tLoss_D: 0.5482\n",
            "[46/500]\tLoss_D: 0.5513\n",
            "[51/500]\tLoss_D: 0.5685\n",
            "[56/500]\tLoss_D: 0.5385\n",
            "[61/500]\tLoss_D: 0.5868\n",
            "[66/500]\tLoss_D: 0.5408\n",
            "[71/500]\tLoss_D: 0.5770\n",
            "[76/500]\tLoss_D: 0.5937\n",
            "[81/500]\tLoss_D: 0.5649\n",
            "[86/500]\tLoss_D: 0.5549\n",
            "[91/500]\tLoss_D: 0.5919\n",
            "[96/500]\tLoss_D: 0.5868\n",
            "[101/500]\tLoss_D: 0.5919\n",
            "[106/500]\tLoss_D: 0.5526\n",
            "[111/500]\tLoss_D: 0.5535\n",
            "[116/500]\tLoss_D: 0.5763\n",
            "[121/500]\tLoss_D: 0.5994\n",
            "[126/500]\tLoss_D: 0.5638\n",
            "[131/500]\tLoss_D: 0.5669\n",
            "[136/500]\tLoss_D: 0.5441\n",
            "[141/500]\tLoss_D: 0.5793\n",
            "[146/500]\tLoss_D: 0.5786\n",
            "[151/500]\tLoss_D: 0.5626\n",
            "[156/500]\tLoss_D: 0.5519\n",
            "[161/500]\tLoss_D: 0.5812\n",
            "[166/500]\tLoss_D: 0.5533\n",
            "[171/500]\tLoss_D: 0.5917\n",
            "[176/500]\tLoss_D: 0.5594\n",
            "[181/500]\tLoss_D: 0.5690\n",
            "[186/500]\tLoss_D: 0.5603\n",
            "[191/500]\tLoss_D: 0.5909\n",
            "[196/500]\tLoss_D: 0.5905\n",
            "[201/500]\tLoss_D: 0.5647\n",
            "[206/500]\tLoss_D: 0.5940\n",
            "[211/500]\tLoss_D: 0.5553\n",
            "[216/500]\tLoss_D: 0.5521\n",
            "[221/500]\tLoss_D: 0.5839\n",
            "[226/500]\tLoss_D: 0.5781\n",
            "[231/500]\tLoss_D: 0.5920\n",
            "[236/500]\tLoss_D: 0.5979\n",
            "[241/500]\tLoss_D: 0.5864\n",
            "[246/500]\tLoss_D: 0.5786\n",
            "[251/500]\tLoss_D: 0.5528\n",
            "[256/500]\tLoss_D: 0.5434\n",
            "[261/500]\tLoss_D: 0.5752\n",
            "[266/500]\tLoss_D: 0.5474\n",
            "[271/500]\tLoss_D: 0.5866\n",
            "[276/500]\tLoss_D: 0.5807\n",
            "[281/500]\tLoss_D: 0.5451\n",
            "[286/500]\tLoss_D: 0.5557\n",
            "[291/500]\tLoss_D: 0.5525\n",
            "[296/500]\tLoss_D: 0.5808\n",
            "[301/500]\tLoss_D: 0.5870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Discriminator Loss During Training\")\n",
        "plt.plot(losses_adam_w,label=\"Loss\")\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7dkeQy5s27Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 2000\n",
        "path = '/content/drive/MyDrive/Internship/training/netD_imagenet1024_w_2000.pt'\n",
        "loss = losses_adam_w[-1]\n",
        "\n",
        "torch.save({\n",
        "    'iter': iter,\n",
        "    'model_state_dict': netD_Adam_W.state_dict(),\n",
        "    'optimizer_state_dict': optimizerD_Adam_W.state_dict(),\n",
        "    'loss': loss,\n",
        "}, path)"
      ],
      "metadata": {
        "id": "F8U1kvd4UuQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}